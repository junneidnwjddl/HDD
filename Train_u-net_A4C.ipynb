{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train_u-net_A4C.ipynb","provenance":[{"file_id":"1ARGyQcZr2Zy2LlIBTgsjC6-90nmvLO86","timestamp":1638455526219}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0WsnX427hCZ","executionInfo":{"status":"ok","timestamp":1638817914313,"user_tz":-540,"elapsed":392,"user":{"displayName":"woojung han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09991882591976136732"}},"outputId":"9450210f-477c-45ce-d84c-b44adf875d4a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"hH1IEUYM66Jj"},"source":["import numpy as np\n","# 데이터 로드\n","X = np.load('drive/MyDrive/Heart Disease/dataset/train/X_train_A4C.npy')\n","Y = np.load('drive/MyDrive/Heart Disease/dataset/train/Y_train_A4C.npy')\n","X_val = np.load('drive/MyDrive/Heart Disease/dataset/validation/X_val_A4C.npy')\n","Y_val = np.load('drive/MyDrive/Heart Disease/dataset/validation/Y_val_A4C.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAc2Kzpy98L3"},"source":["num_train_examples = len(X)\n","num_test_examples = len(X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8AXPBFeq70j1"},"source":["## DataLoader"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2rKIozYELfH","executionInfo":{"status":"ok","timestamp":1638817922017,"user_tz":-540,"elapsed":2984,"user":{"displayName":"woojung han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09991882591976136732"}},"outputId":"b29ed606-7a79-4a3f-fd76-8f853162488e"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}]},{"cell_type":"code","metadata":{"id":"mtHKxYNKDx4D"},"source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n","import math\n","\n","img_shape = (480, 640, 3)\n","    \n","def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n","    \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n","    if width_shift_range or height_shift_range:\n","        if width_shift_range:\n","            width_shift_range = tf.random.uniform([],  \n","                                                  -width_shift_range * img_shape[1],\n","                                                  width_shift_range * img_shape[1])\n","        if height_shift_range:\n","            height_shift_range = tf.random.uniform([],\n","                                                   -height_shift_range * img_shape[0],\n","                                                   height_shift_range * img_shape[0])\n","        output_img = tfa.image.translate(output_img,\n","                                         [width_shift_range, height_shift_range])\n","        label_img = tfa.image.translate(label_img,\n","                                        [width_shift_range, height_shift_range])\n","    return output_img, label_img\n","\n","def flip_img(horizontal_flip, vertically_flip, tr_img, label_img):\n","    if horizontal_flip:\n","        flip_prob = tf.random.uniform([], 0.0, 1.0)\n","        tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n","                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)), \n","                                lambda: (tr_img, label_img))\n","    if vertically_flip:\n","        flip_prob = tf.random.uniform([], 0.0, 1.0)\n","        tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n","                                lambda: (tf.image.flip_up_down(tr_img), tf.image.flip_up_down(label_img)), \n","                                lambda: (tr_img, label_img))\n","    return tr_img, label_img\n","\n","\n","def rotate_img(tr_img, label_img):\n","    rotate_prob = tf.random.uniform([], 0.0, 1.0)\n","    degrees = tf.random.uniform([], -45, 45)\n","    tr_img, label_img = tf.cond(tf.less(rotate_prob, 0.5),\n","                            lambda: (tfa.image.rotate(tr_img, degrees * math.pi / 180, interpolation = 'BILINEAR', fill_mode= 'reflect'), tfa.image.rotate(label_img, degrees * math.pi / 180, interpolation = 'BILINEAR' , fill_mode= 'reflect')), \n","                            lambda: (tr_img, label_img))\n","    return tr_img, label_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0NC6phv7NdO"},"source":["def _augment(img,\n","             label_img,\n","             resize=None,  # Resize the image to some size e.g. [256, 256]\n","             scale=1,  # Scale image e.g. 1 / 255.\n","             hue_delta=0.01,  # Adjust the hue of an RGB image by random factor\n","             horizontal_flip=True,    # Random left right flip,\n","             vertically_flip=True,    # Random up down flip,\n","             width_shift_range=.1,    # Randomly translate the image horizontally\n","             height_shift_range=.1):    # Randomly translate the image vertically\n","     \n","    if resize is not None:\n","        # Resize both images\n","        img = tf.expand_dims(tf.argmax(img,axis=-1),-1)\n","        label_img = tf.expand_dims(tf.argmax(label_img,axis=-1),-1)\n","        label_img = tf.image.resize(label_img, resize)\n","        img = tf.image.resize(img, resize)\n","  \n","    if hue_delta:\n","        img = tf.image.random_hue(img, hue_delta)\n","  \n","    img, label_img = flip_img(horizontal_flip, vertically_flip, img, label_img)                               \n","    img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)                      \n","    img, label_img = rotate_img(img, label_img)\n","    label_img = tf.cast(label_img, dtype=tf.float32) * scale\n","    img = tf.cast(img, dtype=tf.float32) * scale\n","    return img, label_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5J_pESSm7z-q"},"source":["import tensorflow as tf\n","\n","def _process_pathnames(X, Y):\n","    img = tf.convert_to_tensor(X)\n","    label = tf.convert_to_tensor(Y)\n","    return img, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvadCVsN73vJ"},"source":["batch_size = 16\n","\n","import functools\n","import tensorflow as tf\n","\n","def gen_train():\n","    for x, y in zip(X, Y):\n","        yield _process_pathnames(x, y)\n","\n","def gen_test():\n","    for x, y in zip(X_val, Y_val):\n","        yield _process_pathnames(x, y)\n","\n","def get_dataset(#X,Y, \n","            preproc_fn = functools.partial(_augment),\n","            threads = 2,\n","            batch_size = batch_size,\n","            is_train = True):\n","  if is_train:\n","    dataset = tf.data.Dataset.from_generator(gen_train, (tf.float32, tf.float32), ((480, 640, 3), (480, 640, 1)))\n","  else:\n","    dataset = tf.data.Dataset.from_generator(gen_test, (tf.float32, tf.float32), ((480, 640, 3), (480, 640, 1)))\n","  dataset = dataset.batch(batch_size)\n","  print(dataset)\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W5f_F-tu76zX"},"source":["train_dataset = get_dataset(is_train=True)\n","test_dataset = get_dataset(is_train=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkyOAKnE78CG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638817945568,"user_tz":-540,"elapsed":3,"user":{"displayName":"woojung han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09991882591976136732"}},"outputId":"5d9b437f-627e-4832-9c7d-4864d2b8e4a8"},"source":["train_dataset.take(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TakeDataset shapes: ((None, 480, 640, 3), (None, 480, 640, 1)), types: (tf.float32, tf.float32)>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"W69oTTi47_1i"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"a-pt7JNPEhUl"},"source":["from tensorflow.python.keras import layers\n","from tensorflow.python.keras import models\n","from tensorflow.python.keras import losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnNRnHQV8Dme"},"source":["class Conv(tf.keras.layers.Layer):\n","    def __init__(self, num_filters, kernel_size):\n","        super(Conv, self).__init__()\n","        self.conv = layers.Conv2D(num_filters, kernel_size, padding='same', kernel_initializer='he_normal')\n","        self.bn = layers.BatchNormalization()\n","        self.dropout = layers.Dropout(0.2)\n","\n","    def call(self, inputs):\n","        x = self.conv(inputs)\n","        x = self.bn(x)\n","        x = self.dropout(x)\n","        x = layers.ReLU()(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhZ_sHvT8B1f"},"source":["class ConvBlock(tf.keras.layers.Layer):\n","    def __init__(self, num_filters):\n","        super(ConvBlock, self).__init__()\n","        self.conv1 = Conv(num_filters, 3)\n","        self.conv2 = Conv(num_filters, 3)\n","\n","    def call(self, inputs):\n","        conv_block = self.conv1(inputs)\n","        conv_block = self.conv2(conv_block)\n","        return conv_block\n","\n","class EncoderBlock(tf.keras.layers.Layer):\n","    def __init__(self, num_filters):\n","        super(EncoderBlock, self).__init__()\n","        self.conv_block = ConvBlock(num_filters)\n","        self.encoder_pool = layers.Conv2D(num_filters, 3, padding='same', strides=2, activation='relu', kernel_initializer='he_normal')\n","        \n","    def call(self, inputs):\n","        encoder = self.conv_block(inputs)\n","        encoder_pool = self.encoder_pool(encoder)\n","\n","        return encoder_pool, encoder                                         \n","\n","\n","class DecoderBlock(tf.keras.layers.Layer):                                         \n","    def __init__(self, num_filters):\n","        super(DecoderBlock, self).__init__()\n","        self.convT = layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', kernel_initializer='he_normal')\n","        self.bn = layers.BatchNormalization()\n","        self.conv_block = ConvBlock(num_filters)\n","\n","    def call(self, input_tensor, concat_tensor):\n","        decoder = self.convT(input_tensor)\n","        decoder = self.bn(decoder)\n","        decoder = layers.ReLU()(decoder)\n","        decoder = layers.concatenate([decoder,concat_tensor])\n","        decoder = self.conv_block(decoder)\n","\n","        return decoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-muYTAKffBvE"},"source":["class UNet(tf.keras.Model):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        self.encoder_block1 = EncoderBlock(16)\n","        self.encoder_block2 = EncoderBlock(32)\n","        self.encoder_block3 = EncoderBlock(64)\n","        self.encoder_block4 = EncoderBlock(128)\n","        self.encoder_block5 = EncoderBlock(256)\n","        \n","        self.center = ConvBlock(512)\n","        \n","        self.flatten = tf.keras.layers.Flatten()\n","        self.output_linear = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n","        \n","        self.decoder_block5 = DecoderBlock(256)\n","        self.decoder_block4 = DecoderBlock(128)\n","        self.decoder_block3 = DecoderBlock(64)\n","        self.decoder_block2 = DecoderBlock(32) \n","        self.decoder_block1 = DecoderBlock(16) \n","\n","        self.output_conv = layers.Conv2D(1, 1, padding='same', activation='sigmoid', kernel_initializer='glorot_uniform' )\n","\n","    def call(self, inputs):\n","        encoder1_pool, encoder1_skip = self.encoder_block1(inputs)\n","        encoder2_pool, encoder2_skip = self.encoder_block2(encoder1_pool)\n","        encoder3_pool, encoder3_skip = self.encoder_block3(encoder2_pool)\n","        encoder4_pool, encoder4_skip = self.encoder_block4(encoder3_pool)\n","        encoder5_pool, encoder5_skip = self.encoder_block5(encoder4_pool)\n","        \n","        center = self.center(encoder5_pool)\n","        \n","        decoder5 = self.decoder_block5(center, encoder5_skip)\n","        decoder4 = self.decoder_block4(decoder5,encoder4_skip)\n","        decoder3 = self.decoder_block3(decoder4,encoder3_skip)\n","        decoder2 = self.decoder_block2(decoder3,encoder2_skip)\n","        decoder1 = self.decoder_block1(decoder2,encoder1_skip)\n","\n","        outputs = self.output_conv(decoder1)\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U33peOftdNjd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638817961174,"user_tz":-540,"elapsed":7374,"user":{"displayName":"woojung han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09991882591976136732"}},"outputId":"f78f2126-7f4e-4ef8-985f-0eeeb04d16d4"},"source":["model = UNet()\n","outputs = model(tf.random.normal([batch_size, 480, 640, 3]))\n","print(outputs.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(16, 480, 640, 1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QdepTpMDdWMt","executionInfo":{"status":"ok","timestamp":1638817961174,"user_tz":-540,"elapsed":6,"user":{"displayName":"woojung han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09991882591976136732"}},"outputId":"06e950ff-77ea-4847-b532-9bb2108c5e83"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"u_net\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_block (EncoderBlock  multiple                 5216      \n"," )                                                               \n","                                                                 \n"," encoder_block_1 (EncoderBlo  multiple                 23392     \n"," ck)                                                             \n","                                                                 \n"," encoder_block_2 (EncoderBlo  multiple                 92864     \n"," ck)                                                             \n","                                                                 \n"," encoder_block_3 (EncoderBlo  multiple                 370048    \n"," ck)                                                             \n","                                                                 \n"," encoder_block_4 (EncoderBlo  multiple                 1477376   \n"," ck)                                                             \n","                                                                 \n"," conv_block_5 (ConvBlock)    multiple                  3544064   \n","                                                                 \n"," flatten (Flatten)           multiple                  0 (unused)\n","                                                                 \n"," dense (Dense)               multiple                  0 (unused)\n","                                                                 \n"," decoder_block (DecoderBlock  multiple                 2952960   \n"," )                                                               \n","                                                                 \n"," decoder_block_1 (DecoderBlo  multiple                 739200    \n"," ck)                                                             \n","                                                                 \n"," decoder_block_2 (DecoderBlo  multiple                 185280    \n"," ck)                                                             \n","                                                                 \n"," decoder_block_3 (DecoderBlo  multiple                 46560     \n"," ck)                                                             \n","                                                                 \n"," decoder_block_4 (DecoderBlo  multiple                 11760     \n"," ck)                                                             \n","                                                                 \n","=================================================================\n","Total params: 9,448,737\n","Trainable params: 9,441,729\n","Non-trainable params: 7,008\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"2uPx9C_e8QDI"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"hu-RgOik8O1M"},"source":["model = UNet()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPpTZLU78Rff"},"source":["def dice_coeff(y_true, y_pred): \n","    smooth = 1e-10        \n","    # Flatten\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.reshape(y_pred, [-1])\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n","    score = (2. * intersection + smooth) / (tf.reduce_sum(tf.square(y_true_f)) + \\\n","                                            tf.reduce_sum(tf.square(y_pred_f)) + smooth)\n","\n","    return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlI9khEn8V-u"},"source":["def dice_loss(y_true, y_pred):\n","    loss = 1 - dice_coeff(y_true, y_pred)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDo8kls08YL6"},"source":["def bce_dice_loss(y_true, y_pred):\n","    loss = tf.reduce_mean(losses.binary_crossentropy(y_true, y_pred)) + \\\n","          dice_loss(y_true, y_pred)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RjuhuVHW8aA1"},"source":["import os\n","import matplotlib.pyplot as plt\n","optimizer = tf.keras.optimizers.Adam(1e-4)\n","\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                               model=model)\n","checkpoint_prefix = os.path.join('drive/MyDrive/Heart Disease/my_model_A2C', \"ckpt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0ufIs9T8dIN","colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"status":"error","timestamp":1638821200926,"user_tz":-540,"elapsed":20961,"user":{"displayName":"woojung han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09991882591976136732"}},"outputId":"90651c19-f864-4c11-e95b-0bc9beb66f5e"},"source":["from IPython.display import clear_output\n","import time\n","\n","    \n","# save loss values for plot\n","loss_history = []\n","global_step = 0 # step 수 정의 (선택)\n","print_steps = 100 # tf.gradient_tape\n","save_epochs = 5 # tf.gradient_tape\n","max_epochs = 20\n","\n","patience = 5 * print_steps\n","\n","for epoch in range(max_epochs):\n","    #print(\"------------epoch---------\", epoch)\n","    for images, labels in train_dataset: # 데이터 로드 파트\n","        start_time = time.time()\n","        global_step = global_step + 1\n","            \n","        with tf.GradientTape() as tape: # 모델 학습 파트\n","            predictions = model(images, training=True) # [batch_size, 256,256,3]\n","            # [batch_size, 256, 256, 1] - prediction\n","            loss = bce_dice_loss(labels, predictions) # label [batch_size, 256, 256, 1]\n","        # 가중치 업데이트 파트\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            \n","        # 학습 상태 출력\n","        epochs = global_step * batch_size / float(num_train_examples)\n","        duration = time.time() - start_time\n","        if global_step % print_steps == 0:\n","            clear_output(wait=True)\n","            examples_per_sec = batch_size  / float(duration)\n","            print(\"Epochs: {:.2f} global_step: {} loss: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n","                          epochs, global_step, loss, examples_per_sec, duration))\n","            if len(loss_history) == 0:\n","               pass\n","            elif loss_history[-1][-1] < loss:\n","                patience -= 1\n","                if patience < 0:\n","                   break\n","            break\n","        \n","            loss_history.append([epoch, loss])\n","              \n","            # print sample image                     \n","            for test_images, test_labels in test_dataset.take(1):\n","                predictions = model(test_images, training=False)\n","              \n","            plt.figure(figsize=(20, 20))\n","            plt.subplot(1, 3, 1)\n","            plt.imshow(test_images[0,:,:,:])\n","            plt.title(\"Input image\")\n","              \n","            plt.subplot(1, 3, 2)\n","            plt.imshow(test_labels[0,:,:,0])\n","            plt.title(\"Actual Mask\")\n","              \n","            plt.subplot(1, 3, 3)\n","            plt.imshow(tf.round(predictions[0,:,:,0]))\n","            plt.title(\"Predicted Mask\")\n","            plt.show() \n","              \n","      # saving (checkpoint) the model periodically\n","    if (epoch+1) % save_epochs == 0:\n","        checkpoint.save(file_prefix = checkpoint_prefix)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-414d969f2689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce_dice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# label [batch_size, 256, 256, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# 가중치 업데이트 파트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    588\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    591\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    592\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1245\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,32,480,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2DBackpropInput]"]}]},{"cell_type":"code","metadata":{"id":"ZYHiMMDk9iTj"},"source":["# 모델 통째로 저장\n","model.save('./my_model_A4C')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLG6S865imEg"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"TmMuVb0e9JFO","executionInfo":{"status":"ok","timestamp":1638455045054,"user_tz":-540,"elapsed":1012,"user":{"displayName":"woojung han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09991882591976136732"}},"outputId":"89d84839-de88-4f4a-857f-4ac0fda51e66"},"source":["loss_history = np.asarray(loss_history)\n","plt.figure(figsize=(4, 4))\n","plt.plot(loss_history[:,0], loss_history[:,1])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"oz-ew2LCiC6w"},"source":[""],"execution_count":null,"outputs":[]}]}